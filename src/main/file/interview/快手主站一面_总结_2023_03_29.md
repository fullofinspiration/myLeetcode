# 面试自我介绍

面试官您好，我叫张振朋，贵州大学计算机应用技术硕士毕业。 我的工作经历按时间倒序介绍：

## 北京智谱华章（2023.06-2025.04）Java开发工程师
   职责： 智能问答系统开发与技术攻关
   成果：

文档解析：异步批处理+连接池优化，速度提升3倍至200页/分钟

网络传输：Netty零拷贝改造（FileRegion+CompositeByteBuf），吞吐提升40%

向量检索：BAAI/bge+PGVector实现混合查询，千万级数据P99<100ms，支撑日均5W+请求

流程引擎：Spring State Machine开发DAG编排，20+业务流程平均执行时间从15分钟缩至90秒

## 北京字节跳动（2022.04-2022.12）后端开发（飞书股权激励）
   职责： 金融系统风险治理与行情服务建设
   成果：

死锁问题：二级事务状态表+异步补偿机制，死锁率压至0.1%

并发优化：分段锁+本地缓存，热点数据锁竞争耗时从200ms降至20ms

行情系统：WSAPI/File多源接入，K线实时聚合延迟<50ms，冷热分离降存储成本70%
## 老虎证券（2019.01-2021.11）Java开发工程师
   职责： 券商合约/风控系统开发
   成果：

保证金监控：Kafka实时计算资产快照，准确率99.999%

合约服务：Caffeine+Redis二级缓存，TP99从10ms降至2ms

外汇强平：价格波动自适应算法，无效平仓量减少30%，99.95%订单在200ms内执行

# 讲一下做的最满意的工作
核心包装策略：聚焦“多层次混合缓存架构” + 精细化缓存管理 + 系统化思考

问题定性：不是小修小补，而是“系统性调优”
面试话术： “我们观察到期权交易行情的某个关键查询接口P99延迟在高并发时段出现波动，影响了用户体验。我们对整个数据访问链路的延迟分布进行了详细分析（此处可提一下你用了什么监控工具，如Prometheus+Grafana/公司内部监控/Arthas/埋点日志），定位到外部依赖服务/数据库查询是该接口的P99瓶颈。为解决这个问题，我们启动了一个针对接口高性能和高可用的专项优化项目。”
核心方案升华：打造“智能分级混合缓存架构”
面试话术：
“我们的核心优化策略是设计并实施了一个智能分级混合缓存架构，以最大化缓存命中率，屏蔽底层慢查询，从而降低P99延迟。”
一级缓存（L1 - On-Heap）：“针对高频访问且数据量可控（比如配置信息、核心标的少量行情、用户基础信息等）的关键数据，我们采用基于Guava Cache构建的进程内堆缓存。这层缓存访问速度最快（纳秒级），能扛住瞬时最高频的读请求。”
二级缓存（L2 - Off-Heap or Local Disk）: (你这里没做，但可以提到考虑过或作为未来方向) “对于规模稍大但仍需快速响应的数据，我们评估了使用堆外内存（如Caffeine或MapDB利用Off-Heap）或高性能本地磁盘缓存（如RocksDB、LevelDB嵌入式）作为L2缓存的可行性。它能缓解Full GC对堆缓存的影响，容纳更大数据集。”（如果被追问为什么没做，可以说根据当前数据量和性能评估，Guava Cache已满足要求，但为未来预留了扩展接口）
三级缓存（L3 - Remote Cache）： (你这里没做/没提，但高频策略常用) “对于全量的、低频访问或容量非常大的数据（比如海量历史期权K线、深度行情），我们接入了分布式缓存Redis/Redis Cluster（或其他如Memcached、Pika）。利用其大容量和分布式特性。” (注：这是业界最常见的方案，但在你的场景下期权数据使用Guava Cache作为L3也未尝不可，重点是说明原因)。
你的Guava Cache策略就是核心L1/L3 (根据场景定)： “特别是对于低频、量大的期权行情数据，基于其访问模式，我们选择了使用Guava Cache作为应用层本地缓存层（L3），设置了合理的过期时间、并发级别、容量上限和淘汰策略（比如基于Size或Access Time的LRU）。我们实现了高度灵活的CacheLoader和CacheWriter逻辑，处理缓存的加载和失效。”
重点：缓存加载策略： “为了避免缓存击穿，所有缓存层都实现了Single Flight机制（或利用Guava Cache的LoadingCache本身提供的原子加载特性），确保对同一个失效的Key，只有一个线程去执行底层加载（查DB/外部服务）。”
重点：缓存失效策略： “设计了主动（基于时间TTL/TTI）和被动（监听业务变更事件或DB binlog/Canal变更）相结合的失效机制，确保缓存数据的最终一致性。”
缓存命中率监控： “为每一级缓存都详细埋点监控了命中率、加载时间、淘汰数量等核心指标（强调你看数据），Guava Cache本身也提供了CacheStats。”
补充其他通用降延迟“高大上”策略（作为你的思考和未来方向）
异步化与非阻塞：
“对于底层不可避免的慢查询（如部分必须查DB或调用外部服务的场景），我们采用了CompletableFuture/Reactor/RxJava/协程（根据你的技术栈选）实现异步非阻塞调用。结合本地缓存，在异步结果返回前（或超时处理中），优先尝试从缓存中获取旧数据并返回，这能显著提升接口吞吐量和降低P99，保证用户体验的平滑性（即使用户看到的可能不是最新数据，但在多数可接受范围内）。”
批量查询与预取：
“分析请求模式，对于连续请求多个相关数据的场景（如多个期权合约行情），我们将多个独立查询合并（Batch）成一个批量请求发送给底层数据源（DB/服务），显著减少网络交互次数和DB连接开销。并且在加载一个热点数据时，智能地预取相关联的数据到缓存中。”
请求调度与削峰：
“在高并发时段，结合Rate Limiting（限流） 保护底层服务和缓存。对于可延迟处理的非关键逻辑（如部分日志、非核心计算），引入内存队列进行缓冲（如Disruptor）或削峰填谷（比如用Kafka），做异步处理，不阻塞主请求链路。”
结果复用与共享：
“对于计算成本高、数据变化频率低的结果，在本地进行短暂的结果缓存复用（即使是几秒或几十毫秒） ，可以应对突发重复请求。”
序列化/反序列化优化：
“我们评估了接口中涉及的数据结构，并采用了高效的序列化方案（如Protocol Buffers, FlatBuffers, Kryo 或 Jackson Afterburner），或者优化了Java对象的拷贝（如使用Immutable对象、避免防御性拷贝），减少了网络传输和内存转换开销。”
GC优化：
“为了降低堆内缓存带来的GC影响，特别是降低Stop-The-World（STW）暂停时间对P99指标的影响（因为它可能在那一瞬间拖慢所有请求），我们根据缓存占用量调整了JVM堆大小、优化了垃圾收集器参数（比如G1调优），或者监控GC日志以确保其对P99的影响可控。”
成果展示：数据说话+严谨验证
“优化上线后，通过严格的线上灰度发布和A/B测试（如果有），配合实时监控，我们验证了效果：”
“接口P99延迟从原来的[原值]ms显著降低到了[优化后值]ms，降低了约X%。” (突出核心成果)
“缓存整体命中率提升至[命中率]%，特别是优化后的期权数据缓存层，命中率提升效果非常明显。” (核心支撑数据)
“接口吞吐量（QPS/TPS）提升了约Y%。” (可选，相关收益)
“外部依赖服务的调用QPS显著下降Z%，有效减轻了依赖方压力。” (可选，体现系统收益)
总结升华：体现深度思考和技术广度
“这次优化让我深刻体会到：
缓存是优化P99最有效的武器之一，但缓存设计是个权衡的艺术（Trade-offs），需要在内存占用、数据新鲜度（时效性）、开发复杂度和系统复杂度之间找到平衡点。
分级混合缓存策略能够根据不同数据的特性和访问模式，针对性提供最佳性能。
提高缓存命中率是核心，但需要配套的措施（异步、批量、限流、优雅降级）才能真正构建一个健壮的高性能接口。
数据驱动的优化（Data-Driven Optimization） 至关重要，没有监控和详细的数据分析，优化就是盲目的。
即使是看起来小的优化点（如一天完成的缓存策略变更），只要击中瓶颈、方案精准到位，也能带来显著的性能提升。”
# 嗯，你觉得你在做这个的时候碰到了什么问题吗？这是你一个人做的吗？还是你们团队整体的工作。
✅回答示例（结合你的缓存优化项目）：

S (背景)：
去年Q3，我们交易系统的行情查询接口P99延迟频繁突破200ms，用户投诉增多。经监控分析，发现核心瓶颈是数据库查询响应不稳定（尤其在高峰时段）。

T (我的职责)：
作为负责该接口的主开发，我牵头了本次性能优化专项，目标是在2周内将P99降至50ms以下，同时确保资金计算类数据强一致性。

A ：
(行动)
个人主导方案：
通过分析历史调用日志，识别出高频访问的静态数据（如标的元数据）和低频大容量数据（如期权行情）；
提出分级缓存策略：对静态数据用 Guava Cache 实现全内存缓存，对期权行情采用TTL + 异步刷新 的缓存机制；
设计防击穿逻辑：通过 LoadingCache.get(key) 实现单线程加载+阻塞等待。
团队协作关键点：
测试团队：配合构造了历史峰值3倍的压测流量，验证缓存失效时的降级能力；
DBA团队：协助在Redis集群开启 SLOWLOG，定位到慢查询并优化索引；
风控团队：评审缓存刷新策略，确保资金类数据强一致性兜底机制（超时强制穿透读DB）。
R (结果)：

P99延迟从203ms →42ms（下降79%）；
数据库查询QPS下降68%；
上线后零资损事件，获季度技术突破奖。
-R (问题与反思)：

遇到问题：
Guava Cache全内存缓存导致堆压力增大，Full GC频率上升；
期权行情TTL集中失效引发短暂毛刺。
优化措施：
对缓存分桶（拆为10个子Cache），分散失效时间；
引入 Caffeine 替代部分Guava Cache（减少GC压力）；
后续规划：将本地缓存监控数据接入Prometheus，实现命中率自动化告警。
总结：虽然方案最初是我独立设计的，但最终落地是跨团队协作的结果，尤其是在性能和安全的平衡上，团队经验给了关键支持。

# 嗯，那这块的系统设计这块是怎么做的呢？有哪些模块？然后分别是怎么交互的呢？
该设计通过 三层关键解耦 实现金融级稳定性：

本地缓存与分布式缓存分离 → Guava Cache扛高频，Redis保证数据持久化
数据流与风控流分离 → 风险操作直连DB，规避缓存中间态风险
自动刷新与强失效分离 → 普通数据异步刷新，资金数据实时失效
架构本质是取舍：
✅ 用内存换性能（Guava Cache）
✅ 用复杂度换安全（风控直连DB）
✅ 用异步化换吞吐（L2非阻塞刷新）
精准匹配老虎证券“低数据量、高稳定性”的交易业务诉求。

# OK，诶，我看你在那个字节的时间都好像不是很长啊，能说一下原因。
“我加入字节飞书股权激励团队时，项目正处于一个非常关键的攻坚阶段（或者说：业务探索阶段）。在几个月内，我主导了系统风险治理模块，成功解决了分布式转账死锁
问题（死锁率<0.1%），设计并保证了接口的100%幂等性，并主导了行情基础服务的从0到1建设，实现了99.99%的可用性。这段经历非常有价值，锻炼了我解决复杂高
问题的能力和快速交付的能力。”
“后来，随着字节整体业务战略方向的调整，我所在的这块业务发展路径变得不太清晰（或：公司资源重新整合，该业务后续的方向发生了一些变化）。 结合我个人对技
术发展（如：智能问答、AIGC）的长期兴趣，我选择离开了字节，去寻求一个在AI领域有更明确投入和应用场景的平台
# kafka和rocketmq各自的应用场景
选择rocketmq或kafka主要取决于具体的业务需求，系统要求，以及团队的技术栈偏好
kafka的多副本，顺序写，消费者使用拉取模式，在流量高峰时能匀速消费，防止
系统被拖垮，如果应用场景侧重高吞吐的数据处理，如日志收集，实时数据流处理，kafka可能是更适合的选择，
而rocketmq多个topic共同使用一个物理存储，能处理复杂业务的事务消息，保证消息有序性，定时发送，能处理复杂的业务逻辑
## 简介
kafka中包括生产者，消费者，broker，topic，分区，zk（2.8以后改用kraft），
rocketmq包括生产者，消费者，broker，topic， queue，nameserver(提供轻量级的注册和路由功能)
## 存储设计
kafka对每个分区的消息都是日志形式存储在文件中的，并且每个partition都是一个连续的日志文件。所有的消息追加写入，
顺序写提高了写入效率，但是当partition过多时，效率下降
rocketmq所有的topic都顺序存储在commitlog中，提供同步刷盘和异步刷盘机制，为每个队列提供一个消息队列索引文件，
索引文件可以直接根据消息id查询消息信息，rocketmq通过定期清理和压缩处理旧文件
## 高可用
kafka提供副本机制保证broker发生故障时切换到新的副本，控制器负责维护当前的主副关系，使用zk进行领导选举和broker信息的管理，
使用高水位维护最新可读取的消息，保证主备切换时，不会丢失消息
rocketmq通过主从架构，nameserver高可用，生产者发送消息同步复制后ack，保证消息不丢失
参考：https://juejin.cn/post/7361687968518668322

# 能说一下，行情基础服务 比如说在整体设计上是，就是怎么来保证这个可用性呢？就是说在哪些层级都做了哪些事情？
https://www.abelsun.tech/arch/base/arch-y-ensure-high-availability.html#_2-%E6%95%85%E9%9A%9C%E4%B8%8D%E5%8F%AF%E9%81%BF%E5%85%8D
导致系统出问题的原因可能是多方面的，比如：
网络问题：网络连接故障，宽带带宽出现超时拥塞
性能问题：数据库慢查询，Java Full GC，硬盘IO过大，CPU过高，内存不足等
安全问题： 被网络攻击，如DDoS，异常客户请求，如爬虫等。
硬件问题：硬盘损坏，网卡问题，交换机问题，机房掉电

需要对各自的问题做针对的处理
对于网络问题，需要监控网络流量是否异常，对于不可控的网络问题，接口设计需要支持接口幂等
对于性能问题，需要对业务指标和硬件指标做充足的监控报警，对于发现的问题，做针对性处理
对于安全问题，需要使用orm框架，防止sql注入
一般是服务增加冗余部署，最好是跨机房部署
上面问题通用的解决方案是
限流
限流有几种方式：
1 使用令牌桶（桶中容量处理并且每秒增加数据，超过等待），漏桶处理（队列处理，超过丢弃）
2 只返回必要数据，非关键数据无数据或使用旧数据
3 只处理高优用户
4 使用队列异步延时处理
5 服务根据当前流量做扩缩容
降级 对非关键接口，发生故障时限制访问
熔断：提供开启，关闭，半开启状态，关闭时所有接口均可访问，开启时所有接口均不能访问，半开启时放过访问测试，测试没问题时再开启
另外，设计服务为无状态的服务，方便水平扩缩容，代码规范处理，客户端超时重试，接口增加缓存

# 能说一下那个 synchronized 和那个 Reentrant lock
首先这个，这两个锁都是这个悲观锁，然后两个锁都是可重入锁，然后 SYN synchronized 锁是 Java 的内置锁， 
reentry 的 lock 是这个 Java 通过这个 AQS 来实现的一个锁，然后 reentry 的 lock 它是没有这个超时中断的，
它加了锁就必须一直等待， 然后 Reentry 的 lock 它可以进设置一些超时状态，超时之后可以这个进行一个这个中断和提前的结束

然后云春，然后这个 signalize 锁，它这个有这个就是锁的死锁检测，这样在一些异常情况下，它可以打印一些必要的日志排查问题。
原生的 lock 它是没有这个死锁检测的，发现问题可能相对来说难排查一些。然后 synchronize 锁是这个 jVM 层面实现，
它这个每个版本也进行了这个一系列的优化。比如说锁的这个粗粒度的转，就是多个锁转换成一个锁，然后就是进行一个检测，假如说不需要加锁，
实际运行中可能会去掉。还有就是 jVM 的一个从无锁、偏量锁、轻量级锁、重量级锁的一个这个转换，
然后这个 reentry lock，它这个底层通过这个 AQS 实现。假如说需要设置多个条件，然后可能这个人称 log 锁的更相对来说更灵活一些，
一般情况下就是如果能满足要求的话，就是还是尽量用这个内置锁，假如说不能满足需求的话就可以考虑使用 reentrant lock。

# reentrant lock 的那个空屏和非空屏它在实现上是怎么实现的
假如说它是这个，就是这个人创的log，它底层是有一个这个队列，还有一个这个设置的一个 state 的一个状态，还有就是通过 log support 和 log unsupport 来进行一个针对某个线程的这个休眠和唤醒操作，
非公平锁就是它每次新就是新进入的一个请求，它会和这个队列的对手共同来这个设置这个state，这个 CAS 的状态。
设置成功的这个线程就会抢到这个锁，然后公平锁的话，它每次新添加的请求，
它先会入这个队列，只有队列首部的这个元素材会这个设置这个 CAS 的状态。

# 系统设计题,设计一个系统，发朋友圈，看朋友圈的系统
首先最简单的可能就是每个用户有一个按时间倒排的一个列表，
就每次发了只保存在单个用户下，然后看朋友圈的话，嗯，每个用户会拉取这个他所有的这个朋友，
然后嗯，拉取他所有朋友的这个信息，然后按照这个时间，按照时间进行一个这个倒排，然后再进行一个这个数据的展示
这种的话可能就是嗯，用户量大，还有或者某一个用户的发的这个消息，发的这个东西特别多的时候可能会比较慢。

或还有一种方式可能就是进行一个优化，就是每个用户发朋友圈之后，然后在他的这个所有的所有的这个朋友下边会进行一个这个展示，就是，嗯，
在这个朋友下面，然后进行一个数据的也冗余地存储一份数据，然后这样的话数据量可能会比较大，但是每个每一个用户真正查看的时候，
这样它的性能就是拉取和这个排序的这个时间都可以省，节省掉，可以直接进行数据的展示。

## 就他的那个朋友比较多，然后他发的话，他每发一条都会带所有人的缓存一份，对吧？
对对对，我觉得可以对这种朋友特别多的这种，再增加这种特殊的处理吧。就假如说特别多的话，就不用这种其他的人都存储一份了，
就是在专门缓存一份，这个就用户量，比如说超过了一定的数量就单独地存储这样的一个这样一种用户，然后真正的查询这个消息的时候，
既从这个本地查这个消息列表，也从这个特殊的用户，然后这两种数据结合进行一个展示。

# 那比如说可能有些人他这个号已经不用了，或者说就是他是一个长时间不活跃的用户。觉得对于这部分用户，比如说在他不活跃的时间，这些消息怎么处理？比如说他
可能过段时间要上线了，上线的时候可能要看到就是他关注的这些消息，觉得这个这种用户有什么处理策略吗？

消息存储策略
1 分级存储系统
近期消息（如3个月）保存在高性能存储中
中期消息（3~12个月）保存在成本较低存储中
长期不活跃用户（超过一年）保存在冷存储中
2 消息摘要
将消息摘要保存在热存储中

重新激活策略
用户登录时优先恢复最近消息，后台异步加载后续消息
提供加载更多让用户自由选择

总的策略是定期转移消息到对应的存储中
# 如果由于存储限制，系统只能保存用户最近半年的消息（比如聊天记录、动态等），但有些用户可能一年不登录，等他们再次上线时，发现只能看到最近半年的内容，
而更早的消息已经丢失了。这种情况下，如何优化用户体验或系统设计？
确实，存储限制会导致历史消息丢失，影响回归用户的体验。可以采用如下措施：
分层存储："我们可以用冷热数据分离，低成本存储旧数据，需要时再加载。"
智能摘要："对旧消息做摘要或AI提取关键内容，减少存储压力。"
回归体验优化："用户回来时，可以引导他们恢复数据或适应新规则。"
业务策略："还可以通过VIP服务或本地备份弥补限制。"