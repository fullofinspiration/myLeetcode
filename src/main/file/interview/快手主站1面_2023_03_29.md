# 1
行，然后我先介绍一下我们这边业务，我们这边业务是那个快手的PLC，所谓 
PLC 就是说指那个视频的那个左下角可以点的那个地方。是，比如说它是一个
地理位置或者是一个商品，然后可以点的那个地方叫PRC，然后我们这边是负责
说所有的视频都会走，我们是一个通道。然后比如说你刚才跟你说的那个地理位置
或者是一个商品，那都是我们下游的业务方，是一个大概通道型的这么一个一个
系统吧。然后如果你有更详细的问题，我们可以等最后的时候再聊。
# 嗯，行，那就是请你这边自我介绍一下。
好，面试官，晚上好，我叫张振鹏，我下面做一个自我介绍，我硕士是毕业于贵州大学，也是 211 重点高校，然后本科毕业于石家庄学院嗯。我有下面几段工作经历，最近一段是 22 年的4月到年底大概 9 个月的时间，是在字节跳动的飞书部门做股权激励相关的工作，主要负责了两个业务模块，第一个业务模块是负责行权的交易数据的展示，第二负责的页面是，第二个负责的模块是接入了这个行情，提供给这个股权激励的行情的，这个前端的数据展示还有用于计算税务等一方面的数据计算。
接入的话是接入了两个行情的数据源，然后提供一个是实时的行情数据，另外就是一个分时还有日、周、月 k 线的数据展示呃。因为要求就是一个是计算员工的期权，一个是税务，所以对可用性的要求要比较高，大概是 5 个9，然后也做了一些保证高可用的一些方案呃。比如说增加这个缓存的兜底，然后假如说查不到，就设置一个这个，从这个 DB 中捞取这个数据最久的这个行情数据。还有就是，嗯，在一个行情实时性，我们监测行情，假如说一秒钟，一秒钟没有这一秒钟或者一分钟没有达到这个最新的实时数据，我们会自动切换行情的这个数据源嗯。然后第二段工作经历是 21 年的 11 十二月到这个 22 年的三月，大概 4 个月的时间，是在陌陌的直播部门，是做直播相关的工作，主要负责的两个业务模块。第一个业务模块是一个主播 PK 的一个活动，这个活动是为了增加这个用户粘性，选取主，选取平台流量很大的这个 24 位主播，然后在一个直播间内进行一个主播的，这通过送小礼物的方式进行一个主播的 PK 活动。嗯，这个大概是进行了一个一个月左右，然后是选取了一个月中的这个流量最高的晚上，然后每次直播大概是直播两个小时。
然后这个的流水大概是千万，然后 DAU 大概是百万左右，这个的业务实现方式是我们会在这个首先是监听用户的观看时长，然后这个是通过这个，通过有一个直播心跳，我们会监听卡不卡的这个直播心跳，然后满足一定的工作时长，我们会存取这个会首先会在我们的这个 Redis 保存，然后会调用我们的送礼物接口给与给用户下发小礼物。然后在用户送礼的时候，我们也会监听这个送礼物的接口，然后抽取出假如说是当时特定的礼物会给这个用户，会给对应的主播增加用户，增加这个权重，然后两小时结束后会进行一个这个主播的排名，这是第一个业务。
第二个业务是这个直播的一个推荐，嗯，这个是也是为了增加这个用户的粘性，在用户观看直播的时候，然后会根据用户历史的观看历史，然后是从这个推荐中台拉取这个推荐的数据，然后在观看直播的时候进行这个直播推荐，我每天会在凌晨的时候会从推荐中台拉取这个所有用户的观看的直播历史的分值数据，然后。
在用户打开直播间的时候，然后会有一个假如说用户不停地上滑，可能这种 case 需要排除掉，这里可能要一个延时队列，这里是我们用了一个延时 3 秒的延时队列。然后再进行一个推荐，大概是比如说推送间隔 10 分钟，然后推送一天的次数不超过三次，这样大概一个实现第三段工作经历是这个 19 年的1月到这个 21 年的 10 月是大概两年， 10 个月的时间是在老虎证券的券商平台的风控部门做全做风控相关的工作，主要负责了四个业务，第一个业务是我们的标合约标的库的维护，这个是我们的一个基础服务，主要是保存了股票的股票期权、期货的基本信息。比如说中文、英文、繁体字，还有一些和交易相关的，比如说在某个交易上手是否支持可交易？然后我负责的是一个是这个新增加交易品种，比如新增加了这个港股的期权、新加坡、澳洲的股票，还有就是负责了一些缓存的优化，然后也是从缓存从这个 10 毫秒优化到 2 毫秒。
第二个业务是这个外汇强评的业务，这个是我们之前的这个在这个股票的实时强评的基础上，因为我们的股票系统是一个单只入金美金，然后可以支持多个币种的股票，这样其他购买其他币种的股票时，可能它的现金值为负数，为了防止这个汇率波动，可能需要一个自动换汇的流程，这里有做的工作就是监听这个有这个外汇风险的用户，然后计算一个平仓的一个换汇的一个准确的数量。然后调用这个交易接口进行一个下单，然后通过监听 Kafka 的实时交易结果来更新本地平台系统的状态。第三个业务是第三个是这个保证金欠款公式的计算，然后第四个业务是一个借券系统的开发，然后第四段工作经历是 18 年的6月到这个 18 年 12 月底，大概 6 个月是在快看世界的游戏部门做相关的研发工作，然后大概经历是这些面试官。
# OK，诶，我看你在陌陌和那个字节的时间都好像不是很长啊，能说一下原因。
吗？嗯，在陌陌的话可能就是觉得在字节的平台可能更大一些，然后跟之前的股票业务可能相似一些，能够比较好的延续性。然后在飞书的话可能就是。嗯，这个一些就是部门发展可能没有那么多的业务，然后有一些部门也有一些裁员。
# OK，我，诶，我们可以先聊一下那个直播的那个项目，可能先介绍一下。嗯，就是你，你在这个项目中做了什么事情嘛？就是你这个方案的设计。嗯，基于是基于哪些产品的考虑？或者说基于是哪些考虑做了什么事情？OK。
直播这两个事情，就是首先说这个就是 PK 活动，这个包括一个就是活动之前的一个这个预热，然后预热的话是在其他的直播间，然后观看的时候会进行一个这个接口的调用，所以这个接口的访问量是比较高的，所以在上线前也是，所以我们的数据肯定是要加缓存的，然后我们是把所有的相关的主播，相关的图片信息，还有开播，还有展示时间，这些是都存在了我们的 DB 中。然后真正调用的时候，我们是存了这个瓜 cache 的一个缓存，然后因为这些数据基本上除了运维就是运营人员，其他的时间更新是比较低频的，所以保证尽量保证它是一个纯内存操作，就是实际运行的时候没有一些其他的调用 DB 的操作，这里的话是压测进行的一个主要的一个点，然后这里也是压测大概进行 500 万的，这个就是我们的实际的到的 QPS 大概封直接就是最高是 5, 000， 5, 000 左右。然后看最终，最后看这个 CPU 和这个内存占用也没有，就是也是非常稳定，大概都是不超过这个 30% 的。
然后第二个是就是观看。看直播，然后送礼物，然后最终增加礼物这个地方因为就是使用了这个卡夫卡，然后它是这个天然的一个限流的一个工具，所以这里是没有做这个流量的压测，然后实际中就是我们还有就是调，最终就是增加礼物的这个接口，然后它这个接口它是不支持批量调用，就是我们每观看 30 秒钟它会送 30 个礼物，然后这里的话就是感觉这些礼物就是因为也是不花钱的免费的小礼物。所以这里我们相当于做了一个这个降级，就是假如说没有发失败，这个就允许它失败，只不过就是用了一个异步的方式，然后最多就是发一个告警，但是不影响后续的流程。然后诶，嗯嗯。
# 诶，这个我打断一下，那个你们拉到的那个你们是先拉到观众的这个推荐数据，对吧？
嗯，这个是 PK 那个观众的数据，可能是另外一个业务还没有详细展开。那个就是一个预热的。就是因为有 24 位主播，又包含这个图片信息，还有一些文案信息，就是它的数据量比较大，所以就是加了一个本地缓存哦。
然后那我继续，没什么啊？然后第二个业务是这个观看直播的这个推荐，然后，嗯，这个的话，一个就是刚刚讲的就是不停上滑，可能就是他有，就是要保证有一个延时来进行一个推荐，要不然他会一直推荐。嗯，这个是用的已有的这个中间已有的这个内部搭了一个延时队列的工具，具体就是调用卡夫卡，然后延长一段时间，再消费这个卡不卡的这个消费。然后，嗯，其他的就是一个是保存这个就是监，还有就是这个监听这个用户观看的这个心跳，因为它有可能是卡不卡的重复数据？所以我们一开始是用这个，用了这个Redis，然后进行一个设置，一个大概 3 秒的时间，然后用来做一个这个数据的去重。然后，嗯，真正去重之后，我们嗯会就是，嗯这里也做了一个，嗯，就是有一个数据兜底的操作，就是假如说嗯没有今天的数据，然后就取昨天的数据，假如说今天昨天的数据都没有，我们会这个不进行一个推荐。然后我理解。
# 打断一下，就是说就是那个用户观看直播，进行直播推荐的这个项目是相当于你们去拉了一些这个用户的数据，然后通过在特定的时候，通过一定的策略把它插入到那个已经提供好的一个 feed 流里，对吧？
大概的流程是这样，只不过就是那个 feed 流的方式，我们就是直接实时推荐了，也没有一个流的这个方式。
# 诶，那就是说那个相当于推荐的东西直接是从你这出的，是吧？就没有别的出口，就他刷到那个视频是直接从你这走。
对，就是用户在观看直播的时候，我们会判断假如说在直看直播的时候才会进行这个推荐，假如说推荐了会这个在本地进行一个这个记录。
# OK，嗯，你刚才，诶，你们这个为什么是用的Kafka？没有用Rocketmq。
我们的中间件只有这个卡夫卡 Rocket MQ 是没有的。
# OK，那就是相当于他先收到之后，在本地放到一个那个延迟队列里，然后再进行一个消费。那比如说那个集群在上线的时候，这个事件会丢吗？
# 诶，你们这个消费的集群大概是一个什么样的规模。
我们大这个大概是 30 台机器左右，就是消费者的这个机器数量，对吧嗯？嗯， 30 台左右，嗯，对对对。
# 看一下，你能再介绍一下那个，那个就行情基础服务，这个嘛？就是说我刚才的理解是说你们的这个数据源是只用来查看吗？就是搭建行情基础服务的这个。
那它会涉及到说，比如说我要行权的时候的那个计算吗？是用你。
# 能说一下，比如说在整体设计上是，就是怎么来保证这个可用性呢？就是说在哪些层级都做了哪些事情？
首先就是我们接入了这个两个行情数据源，这个就是保证这个数据的这个可用性，就是呃。其次可能就是一个监控系统，我们每次保存的时候都会存一个这个当前的这个保，我们每次保存之前先会进行一个数据校验。就是比如说它的这个高开低收，它的这个数据合法性，只有在数据合法的时候我们才会存入系统，并且把这个存入系统的这个时间作为这个数据的更新时间。然后同时会每次查询的时候会查询这个更新时间，假如说这个时间和当年时间的间隔比较大，会这个自动切换到这个另外一个数据源，同时也会发出一些告警的动作，然后真正告警之后会这个排查具体的原因呃。实际上的话实际运行的时候也会这个加一个本地比较短的缓存。比如说 1 秒的一个缓存的一个一个兜底，然后假如说我们也会存储这个每天的一个这个备份数据，防止这个 Redis 挂了，同时又从上手取不到数据时，会通过这个当日的这个收盘价，然后作为一个兜底数据，这样保证也不会有这个太大的误差来保证这个接口的可用性。大概是这些。
# 那就是说实际上它是相当于牺牲了一致性来保证的这个可能性。
嗯，对，这个。
# 那这个在行权的时候不会由于这个误差导致算的钱有出入吗？
嗯，这个和这个，嗯产品也进行过这个沟通，就是假如说因为我们的数据就是发生异常的情况，又就是想到的就是 Redis 挂掉，同时我们的上手的这个数据源也没有，然后这个时候使用这个当天的这个数据，这个当天的这个兜底数据，这种情况下一般就是一些大公司除了股价大涨大跌，这个误差不会特别大，除非有一些就是比如说股票的拆合股，这种情况下可能会有一定这个误差就比较大，这个可能就是。嗯，我们也会设置了一个开关，假如说当天有这个拆合股的情况，我们会提前把这个开关设置成一个，就类似于严格模式，就是它不使用这个兜底操作，相当于就是牺牲了可用性来保来这个达到这个数据的正确性。o。
# OK，你在飞书的时候用的是 Java 吗？还是。
# 那我们我再稍微问一点 Java 的这个相关的知识吧。好的，嗯，我们就先聊一下那个 Java 的锁，能说一下那个 synchronized 和那个 Reen reentrant 那个lock。
别吗啊？好的，首先这个，这两个锁都是这个悲观锁，然后两个锁都是可重入锁，然后 SYN synchronized 锁是 Java 的内置锁， reentry 的 lock 是这个 Java 通过这个 AQS 来实现的一个锁，然后 reentry 的 lock 它是没有这个超时中断的，它加了锁就必须一直等待，然后 Reentry 的 lock 它可以进设置一些超时状态，超时之后可以这个进行一个这个中断和提前的结束。
然后云春，然后这个 signalize 锁，它这个有这个就是锁的死锁检测，这样在一些异常情况下，它可以打印一些必要的日志排查问题。原生的 lock 它是没有这个死锁检测的，发现问题可能相对来说难排查一些。然后 synchronize 锁是这个 GVM 层面实现，它这个每个版本也进行了这个一系列的优化。比如说锁的这个粗粒度的转，就是多个锁转换成一个锁，然后就是进行一个检测，假如说不需要加锁，实际运行中可能会去掉。还有就是 GVM 的一个从无锁、偏量锁、轻量级锁、重量级锁的一个这个转换，然后 SYN synchronize，然后这个 reentry lock，它这个底层通过这个 AQS 实现。假如说需要设置多个条件，然后可能这个人称 log 锁的更相对来说更灵活一些，一般情况下就是如果能满足要求的话，就是还是尽量用这个内置锁，假如说不能满足需求的话就可以考虑使用 reentrant lock。
# 诶，那你知道那个 reenatrantlock 的那个公平和非公平它在实现上是怎么实现的？
嗯，假如说它是这个，就是这个人创的log，它底层是有一个这个队列，还有一个这个设置的一个 state 的一个状态，还有就是通过 log support 和 log unsupport 来进行一个针对某个线程的这个休眠和唤醒操作，非公平锁就是它每次新就是新进入的一个请求，它会和这个队列的对手共同来这个设置这个state，这个 CAS 的状态。设置成功的这个线程就会抢到这个锁，然后公平锁的话，它每次新添加的请求，它先会入这个队列，只有队列首部的这个元素材会这个设置这个 CAS 的状态。
# 你们的数据量大概是什么规模？可以就是你随便说一个。
嗯，飞书的话大概是就是内部员工 10 万左右，然后。嗯，这个，嗯就是在这个陌陌的话大概是百万用户，在老虎证券也是百万用户量。
# 那你们的表一般就比如说你们表大，比较大的表它的数据量是一个什么地方？就是记录的条数。
一般我们的那个订单表就是在老虎的时候，它的订单表大概是这个 1, 000 万左右，这是最大的了，是整体的这个量级，是吧呃？就所有用户的这个订单表的这个数量，嗯，是没有做的。
#  然后我们聊一个设计题，就是设计题就是想让你看看设计一个朋友圈，然后它主要两个功能，一个是发布，一个是那个拉取，就是一个是发朋友圈，一个是看朋友圈的，对吧？嗯，设置这么一个系统。
首先最简单的可能就是每个用户有一个按时间倒排的一个列表，就每次发了只保存在单个用户下，然后看朋友圈的话，嗯，每个用户会拉取这个他所有的这个朋友，然后嗯，拉取他所有朋友的这个信息，然后按照这个时间，按照时间进行一个这个倒排，然后再进行一个这个数据的展示。
这种的话可能就是嗯，用户量大，还有或者某一个用户的发的这个消息，发的这个东西特别多的时候可能会比较慢。或还有一种方式可能就是进行一个优化，就是每个用户发朋友圈之后，然后在他的这个所有的所有的这个朋友下边会进行一个这个展示，就是，嗯，在这个朋友下面，然后进行一个数据的也冗余地存储一份数据，然后这样的话数据量可能会比较大，但是每个每一个用户真正查看的时候，这样它的性能就是拉取和这个排序的这个时间都可以省，节省掉，可以直接进行数据的展示。
# 对，那比如说有一个人他的朋友特别多，然后他发一个消息，可能要在所有他的朋友那融一份，对吧？对，就是能怎么优化一下这种情况，就说可能不想用这么大的开销。**
嗯，优化一下可能还是两种方式结合使用，就是假如说你刚您刚刚说是一个用户发的数，发的东西特别多，是吗？
# 就他的那个朋友比较多，然后他发的话，他每发一条都会带所有人的缓存一份，对吧？
对对，我觉得可以对这种朋友特别多的这种，再增加这种特殊的处理吧。就假如说特别多的话，就不用这种其他的人都存储一份了，就是在专门缓存一份，这个就用户量，比如说超过了一定的数量就单独地存储这样的一个这样一种用户，然后真正的查询这个消息的时候，既从这个本地查这个消息列表，也从这个特殊的用户，然后这两种数据结合进行一个展示。
# 那比如说可能有些人他这个号已经不用了，或者说就是他是一个长时间不活跃的用户。觉得对于这部分用户，比如说在他不活跃的时间，这些消息怎么处理？比如说他可能过段时间要上线了，上线的时候可能要看到就是他关注的这些消息，觉得这个这种用户有什么处理策略吗？
可能再增加一，就是再增加一个这个，嗯观看的数量，就是，嗯，假如说他这个就相当于观看之后这条消息就无用的话，就是看追加的这个队列追加的这个消息的数量，假如说数量过多的话，就暂时就不追加了，然后真正等他刷完的时候，嗯，再进行再刷的时候，也通过这个拉的一个策略就直接再拉一些他的这个最新的消息。
# OK，那比如说可能由于存储的一个限制，可能我们每个人只存了近半年的消息，有这种情况可以从其他角度。进行一个优化，比如说他可能他上线已经隔了一年了，但是说那个我们的消息，嗯，可能就是在这种公共的地方可能只存了半年的消息，嗯。
就是存了代码的消息。
# 觉得可以从其他角度，比如说从其他角度去看怎么解这个问题？
数据量越来越多了，嗯。
就因为都是这个最新的数据，可能就是，嗯嗯嗯，有一个那个定时，类似于定时任务，就每次就把超过半年的进行一个这个清空，然后对他如，就不论他看不看半年之前的消息，都给他这个清空掉，然后它不活跃的话，我理解就是个定时任务，这定时清空一些不长就是历史的消息。
# 就这个吧，找一个数组中第 k 个最大元素。
呃。这个你就可以用那个快快排，就是某一个元素，就是假如倒一个位置，就是最终左边的都比它大，右边的都比它，左边的都比它小，右边的第 k 个最大元素，那就按倒序或者是正序，然后是倒数第 k 个元素，保证最终的它的这个位置是这个 n 减，就是第索引值是 n 减k，然后最终得到了这个结果。
# 嗯，就是您那边那个 feed 就是一个商业化中台，就是我我还是不理解，就是有什么就具体的业务可以再说一下吗？
OK，这边是那个，你是看到那商业生态的这个名字，是吧？对对对，我，我是，我们这边不是那个商业化，我们是主站，我们在主站下面，然后是和商业化是两个大的部门，然后我们这边就是我们是主站下面的商业生态，所以和那个商业化没关系。然后那个就是目前用人的这个业务，是说那个它是一个通道型的一个一个服务，然后它下游大概可能有几十个下游。然后你看到手机上你每刷到一个视频，他都会来请求这个服务说说，请求这个服务说要不要出这个PLC？然后要出的话出哪个业务方的PLC？就你可以理解这个视频上可能有多个信息，比如说他既挂了地理位置，就是说我视频上有地理位置，然后我又挂了一个那个商品，比如说一挂了一个小黄车。
嗯，比如我想卖这个商品，我挂在地理位置，然后他就会决策说就是请求打到 PLC 这个服务之后， PLC 会去请求下游的服务，说哪些下游告诉我要出服务，然后我这边根据优先级，根据相关性做一个判断，说要出哪一个PLC，就是说它，因为它那个左下角的位置只能出一个东西嘛。嗯，就是判断说要出哪个东西，然后整体就是一个通道，是一个通道型的一个服务。然后会做一些基本的过滤，然后会有一些相关性的策略，然后会有一些那种优先级的策略。
# 就它除了展示，比如说购物车展例、展示位置，还会展示其他的吗。
下游大概有四十几个业务，可能比如说你看的就是影视卡，就你点进去是个视频，然后它可能还是个小程序一样，嗯，然后或者是一个快影，就是一个剪辑的，他下游有 40 多个业务，嗯，对。
我就几十个业务。明白，对，了解。
然后也是一个高 QPS 的系统，我们前段时间统计大概是 40 万BPS，就是他做了很多缓存上的事情，缓存上的优化相关的。嗯嗯，那还是挺高。嗯，就因为那个快手的，那个 t 的都会来请求这个服务。对，他的 CBS 是变高这块特点吗？