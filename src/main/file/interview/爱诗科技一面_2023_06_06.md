
# 爱诗科技 2023.6.6 做AGI视频
CEO是中科大 主要围绕项目做提问：
##  项目挺丰富，目前是做这么
##  想了解一下你这些项目里面你觉得做得最好的是哪个业务？
回答推荐
## 这块你可以介绍一下这个推荐的算法是你这边做的吗？
这个不是，这个是已有的，这个是推荐中台自己做的算法，只不过数据是推送过来的。
## OK，你这边做的是哪些工作啊？
这边做的是就是使用这个推荐过来的这些数据做一个推荐的工作。嗯，比如说，嗯，就是数据是以什么样的格式，然后进行存储，然后真正用户在观看直播的时候，
然后假如说满足一定的条件，比如说进直播间满 30 秒，然后因为有不停上滑，这个时候不能推荐，所以要加一些延时队列，只有在稳定地在当前直播间观看才进行
推荐，同时还有一些推荐的一些策略，就是推送间隔的话，推荐间隔的话是 10 分钟，然后每天最多推荐 3 次。就是然后推荐的话会有一个比如说当天的话是推荐过
的就不能推荐，还有的话就是按照这个分值由高到低进行推荐，然后还要有一些更细节的，比如说假如说这是这个推荐的主播正在开播，然后就过滤到这些主播，就是一些
业务的处理嗯。
## 嗯，那这块的系统设计这块是怎么做的呢？有哪些模块？然后分别是怎么交互的呢？
嗯，首先就是用户，有用户就是已有的，已有的架构是，就是用户从进房时刻开始，然后每隔 30 秒会有一个心跳事件，这心跳的一个卡夫卡事件，
这个是已有的这个组件，然后真正设计的时候是应用了这个消息来做这个业务的实现。
比如说就是因为推荐推荐中台的这个数据相当于是一个离线数据，所以真正设计的时候就是我们自己申请一个 Redis 库，然后每天由他们把这个数据推送给我们，这个
数据的格式是由我们这边来定义的
，是一个哈希 k 的话就是，嗯，用户的 ID y，然后他的这个 y 的话是一个这个一个是主播，一个是对应的主播，还有一个就是对应的主播的分值。
还有的话就是还有一些兜底策略，比如说一些推荐中台有一些异常，然后导致当天的数据没有及时地推送，会使用这个昨天的数据呃。真正进行这个推荐的时候，
就是先监听这个用户的进件，假如说当前正在进行，当前这个已经进房，同时停留在当前的这个直播间满超过了 3 秒钟，然后就会走这个后续的流程，这个超过 3
秒钟是通过一个已有的这个延时队列的中间件去向那边发送一个 Kafka 的一个消息，然后再通过这。这个 consumer 接收到这个消息，再进行这个后续的业务处理，
真正的业务处理每个主播 ID 的这个首先卡不卡的消息是有可能是重复的，这里是在消费端，首先是通过 Redis 的 setns 进行一个消费端的一个消息的去重，
真正去重之后会根据这个用户已经推荐过的历史，然后首先把这个用户的推荐的数值，然后进行这个加载到内存中，然后再根据过滤掉已经推荐的历史，
然后再按照分值还有这个没推荐过的主播。然后再进行这个排列，选取这个最高的，然后将这个主播标记成已推荐，同时给这个客户端下发一个推荐这个主播的一个消息，
用户的话就是不管点不点都认为这次已经是一个推荐过的一个主播了。然后这个每天的话，这种推荐历史会重置，就是会每天一刷新，其他的方案应该就没有了。

备注：这里问的系统设计，但是回答的是业务细节
## 嗯，你觉得你在做这个的时候碰到了什么问题吗？这是你一个人做的吗？还是你们团队整体的工作。
这个是一个人做的。嗯， Pod 的应该也没有什么问题。嗯，这个没有遇到什么问题。

## 有遇到什么问题，是吧？对，那你们怎么观察到这个一用户平均这个是怎么统计出来的呢？
嗯，您说是用户的观看时长是吗？对啊，这个是，嗯，我们就是一开始是，就是真正上线的时候是灰度上线的，
然后是选取了这一部分数，这一部分这个用户的每天的这个观看时长是有所统计的，
然后灰度上线的大概是一个月左右的时间，通过观察是用户，就是总体的这个观看时长是这个有所提升的，
和这个一个月之前相比，这种最终的话也是这个全量上线的。这个所有的这个用户。

## 在老虎证券这边，这个你写 P99 是降低了很多，这个是怎么做的？
嗯，这个一开始是用的那个 Golang cache，它加载内存的方式是通过用户线程来调用的，
然后这种方式的话就是，嗯，他有可能就是一个是，就是他下一次使用的时候，然后这个缓存是 10 分钟前使用的，
这个缓存就过期的。另外的话就是使用瓜cache，然后有可能就是进入老年代，然后这时候发生 full GC 的这个时间会影响这个 TP 99 的平均耗时。
所以解决方式是使用那个 concurrent map 代替了这个瓜cache，然后系统在启动的时候会加载所有的数据到内存中，所以基本上所有的数据都是这个命中缓存的，
就没有从调用这个 DB 的操作，
从这种方式的话是缓存的这个命中率也从这个 60% 上升到这个92%。
然后因为就是使用了 concurrent map，它们即使进入到老年代也不会这个，因为数据假如说用广告 cache 它会一段时间过期，然后会只能通过 full GC 会回收，
full GC 的话会有一定的耗时。现在的话就是使用 concurrent map，它老年代就不设置过时，过期时间它就可以一直常驻在这个老年代当中，然后这样会尽量减少这
个负 GC 的发生。然后缓存的更新是通过订阅那个 MySQL 的binlog，就是使用那个阿里的中间件 Canal 来这个进行缓存的这个异步的一个刷新，这样保证数据实时性
的同时也保证了这个缓存的命中率。这样的方式来这个使 TP 99 的耗时减少到 2 毫秒。

## 你平时写什么语，用什么语言多一点呢。
写 Java 多一些。
## 算法题
在麻将中，胡牌需要满足特定的牌型组合规则。题目实现的是一个简化版本的胡牌判定算法，规则如下：

牌类型：仅包含数字牌（1~9的万/条/筒，无风牌、箭牌等特殊牌）。

牌数：固定14张牌（标准麻将胡牌的手牌数）。

胡牌条件：

一对将（眼）：两张相同的牌（如[1,1]）。

四组顺子或刻子：

刻子（AAA）：三张相同的牌（如[2,2,2]）。

顺子（ABC）：三张连续的数字牌（如[3,4,5]）。


