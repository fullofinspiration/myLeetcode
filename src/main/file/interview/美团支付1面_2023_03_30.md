# 知识库系统的方案设计，以及遇到的难点
知识库系统包括文档解析，向量化，入库，向量召回，知识召回，提交大模型，
不同的文档使用不同的开源库处理，doc使用fitz库，网页浏览器使用模拟浏览器库，
向量库使用pg，具体对话内容使用mysql存储

流程编排工具自研流程数据结构，包括agent模式和连线模式。
遇到难点包含：
文档解析速度慢的问题
上线后客服反馈，部分文档解析速度过慢，比如超过100w字，文档解析

# 面对复杂业务，架构有什么通用思路
1 拆分，降低架构复杂度
2 认知抽象，架构模式有通用性
3 平台化，中台化系统衍化建设


# 谈谈对DDD模式理解
https://tech.meituan.com/2017/12/22/ddd-in-practice.html
ddd是为了应对业务的复杂性，如果所有的业务耦合在一起，会增加业务复杂性，
不方便业务扩展和快速迭代
当然更多的场景是快速迭代，我们可以通过敏捷开发，测试驱动，持续重构，来保证代码
质量，但是这种方式的缺点是，每次抽象出来的类，抽象出来的模块对应的只是技术含义，
并没有业务含义，需要更加易懂，方便迭代的方式组织代码，ddd是比较好的方式
初始阶段开发时，通过action/service/dao方式，业务逻辑全部揉在service中，
java面向对象的特性完全得不到体现
通过解耦的方式，划分出来多个模块，并且模块能够高内聚，低耦合
通常按照微服务的架构，将不同的业务划分到不同的微服务中，ddd的核心诉求是将具体的业务
映射到系统的架构上，
在开发具体业务时，需要对具体业务进行建模和分层，划分好各自的业务和职责边界
首先根据业务划分出限界上下文，每个限界上下文中将实体和值对象按照业务聚合在一个
聚合根中
实体，包含一个唯一id和该实体对应的业务属性，以及对该领域进行建模，
根据产品文档和对应的业务有哪些动词提取出对应的方法
值模型，不包含唯一id，只包含对应的属性，这种方式在实践中，因为产品需求持续变化，
属性也持续变化，所以使用领域模型更多一些
聚合根：将有相关关系的领域模型聚合在一起 
facade层：对外部系统的访问增加防腐层，进行领域访问和数据转换
这样能方便的扩展业务

# 谈谈你对设计模式的理解
https://juejin.cn/post/6976573521228988446
编写代码时需要遵循几个原则：
solid原则
single responsebility principle 单一职责
open-close principle 开闭原则 对修改关闭，对扩展开放
里式替换原则：父类可以被子类替换，子类可以对父类方法扩展，入参更宽松，返回参数更严格
interface segregation principle接口隔离原则 不同的业务使用不同的接口
dependency inversion principle 依赖翻转原则，具体业务值依赖于接口，而不依赖具体实现
简单原则 代码清晰易懂 keep it simple and stupid,不要过早优化，定期重构
最少原则，依赖的接口，参数越少越好，只使用最少的参数
表达原则：代码具有可读性，代码即文档
分离原则：相同的东西放在一起
契约原则 遵循方法名实现的含义 API命名，接口幂等性，接口分版本



# 如何设计一个高并发系统
1 页面静态化
2 使用cdn能够访问距离地理位置最近的节点
3 使用缓存，分布式缓存，内存缓存
4 数据库 读写分离
5 数据库分库分表 解决的是连接数不足，检索耗时过长，io和cpu瓶颈 单表数据量过大，即使有索引访问也很慢 （分库可按业务分离，也可以按用户id做一致性哈希分离
6 数据库使用索引，索引优化：
6 业务优化，非关键业务逻辑异步，使用线程池并发执行
7 使用mq接口异步化，削峰填谷
8 池化思想
数据库池化比如hikari
9 批处理
数据库查询，RPC接口调用尽量使用批处理
10 集群化处理
数据库集群
中间件集群
中间服务器集群
11 负载均衡
使用nginx做负载均衡等，并使用常见的负载均衡算法：轮询，权重轮询，ip hash，最小连接数，最小响应时间
12 限流
实现方式：nginx 或者 redis
限制单个用户，单个ip，单个接口，加验证码
使用nginx配置ip限流，使用redis做用户的限流
13 降级
Hystrix 和 Sentinel
15 异地多活
16 压测
17 监控告警
# explain如何做索引分析
https://www.nowcoder.com/discuss/412261817077190656
1 id列： ：值越大，执行优先级越高
2 select type列:select语句的查询类型
    simple 简单查询
    primary 主查询
    subQuery 子查询
    union 联合查询
    union_result 联合临时表查询
3 table列 ：表名，表别名， 临时表名
4 partition列：未知
5 type列：表连接类型或数据访问类型：表之间通过什么方式连接，通过什么方式访问数据
    system：表中只有一行数据
    const：主键或唯一索引
    eq_ref: 表连接使用了主键或唯一索引
    ref: 使用非唯一索引查询
    ref_or_null: 非唯一索引，值可能为null
    index_merge: 使用多个索引并合并
    range：使用索引范围查询
    index： 使用索引扫描
    ALL: 全表扫描
6 possible keys列：可能用到的索引，实际查询不一定用到
7 key列：实际使用的索引
8 key_len列：索引长度
9 ref列：where语句或者表连接中与值比较的参数，包括const，func，具体字段名
10 rows列：实际扫描的行数
11 filtered列：过滤的表行百分比，越大效率越高
12 extra列：
    using where 使用条件搜索，并没有用索引
    using index：使用覆盖索引，不需要回表
    using filesort：使用外部排序，没有使用索引排序
    using temporary 使用临时表
    using join buffer 使用连接缓存区存储数据
    using index condition 使用索引下推访问特性

什么是覆盖索引：将所需数据建在索引上，查询索引即可得到所需数据，减少回表查询
索引下推：在存储引擎层做数据的条件筛选，只有符合条件的数据才会筛选出来交给服务层
核心使用场景：联合索引下，第一个索引为范围，第二个索引为过滤


# 索引失效
https://mp.weixin.qq.com/s?__biz=MzkwNjMwMTgzMQ==&mid=2247491626&idx=1&sn=18fc949c06f04fe8f4c29b6fc5c66f9c&chksm=c0e838c2f79fb1d45c6f9b2ab188bb4663414690bab0718a7d46beb875e6b83e5e67ec27d2ff&token=902535653&lang=zh_CN&scene=21#wechat_redirect
1 非最左前缀匹配
2 索引列上有计算
3 索引上使用函数
4 使用类型不同
5 like左边包含%
6 列对比
7 or的某一列没有使用索引
8 普通索引使用not in not exist会失效
9 order by时没有加where limit
10 对不同索引做order by
11 联合索引有不同的排序规则
# b+树
https://juejin.cn/post/7050722587961688094
## 计算层数
每个页大小是16k
叶子结点存储数据，非叶子结点存储范围和指针
每个主键8字节（bigint），指针6字节，叶子结点假如一条数据1k，一个叶子结点16条数据，
2层（第一层为根节点）， 则数据量为：16k/14*16=1200*16=20000条数据
3层数据量为1200*1200*16=2000w条数据
## b+树结构是什么，聚簇索引和非聚簇索引都是使用b+树吗
每层包含上确界，下确界，文件头，页头，页尾

## 什么时候使用b+树，什么时候使用那个一层一层的数据结构（我忘记叫什么了），b+树的使用场景是什么，为何不用b树
b树是非叶子结点也会存储数据，这样层级会变多，会有多次io
hash不能范围查找，也不能排序
## 为何使用自增id
如果使用自增id，则只有在页满了才会新建一页，如果是非自增id，那会有多个页比较分散，会耗费系统空间，在数据分散时，插入数据
时，会频繁的页分裂和旋转
## 尽量使用长度较小的主键，这样能存储更多的数据

# 系统具体干什么？然后这其中你负责了哪些东西？以及有什么就做完之后这个事对于整个业务来说有什么效果上的收益吗？
一个知识库系统，一个流程编排系统都有哪些收益
知识库系统：做文档解析，召回，查询前数据拼接
流程编排：有向无环图结构的设计，对话整个流程，状态流转，临时数据保存，数据流转
从零到1搭建了知识库系统和流程编排系统，知识库应用创建50w+，文档数超过千万个，
我负责后端的方案设计，数据存储和数据层面设计和编码的实现

# 方案设计这部分能简单介绍一下，就是你具体设计的审就是面临的什么问题，是怎样的方案解决了什么的问题
数据异构性：支持不同的文档解析
语义理解困难：传统检索无法查询出关键信息
知识碎片化：分散在不同文档中，难以有一个结构化的数据

1 分层存储：
原始文档库（使用cdn存储）
向量化存储：向量化内容
结构化存储：文档内容+知识库，文档id，文档大小等

2 混合检索：
解决方案：使用多种方式解析数据：对于不同数据使用多个开源库解析多种格式的文档，然后使用下面的方式：
1 解析文档内容后，使用多种策略对文档切片和入库，使用开源相似度算法进行数据召回
2 使用es进行存储和关键词检索
3 大模型进行总结，总结后内容作为切片内容
结果融合与排序：
多路召回结果去重，基于相关性重排序，个性化的结果调整

使用场景：企业智能知识中枢，医疗专业知识库，旅游专业知识库
该方案通过大模型的理解能力与结构化存储检索技术的结合，实现了从文档存储到知识服务的质变

# 方案剩下具体做哪些事，以及在你整个这个方案里面有哪些难点
难点1：多个数据源提供统一的查询方式
开发统一适配器层，支持常见格式解析
使用大模型将数据格式化，比如将表格转换为结构化数据
难点2：非结构化数据提取
采用NLP实体识别技术提取关键要素
利用大模型进行语义分块和摘要生成
构建自动化标注流程
难点3：用户知识查询和知识之间如何关联
查询理解增强：意图识别模型，查询改写，关联历史对话处理
混合检索系统（关键词检索+向量检索）
召回知识过滤+排序
难点4：召回效果量化
用户反馈实时手机
A/B测试框架

# 系统稳定性(高可用)方案设计
https://developer.jdcloud.com/article/3894 非常好的文章
1 方案评审
研发，测试，产品，业务方都参与项目评审，阅读文档后提问题而不是讲方案的方式来进行
2 技术方案评审
技术方案是工程技术，不是科学探索，所以要参考业内最佳实践
技术方案要力求简单，避免过度设计
将复杂问题拆解为简单问题
关注的不仅仅是业务实现，更关注的是架构，性能，质量，安全，即保证打造一个高可用的系统
技术评审关注点
2.1 限流
漏桶算法 漏桶算法请求过多容易被拒绝
令牌桶算法 令牌桶算法能够削峰填谷
2.2 熔断降级
熔断是为了防止系统被过多请求拖垮，系统资源一直被占用，造成雪崩效应，一般有现成的中间件，比如Hystrix
2.3 接口超时
分布式的难点就是不可靠的网络，超时类似于一个熔断策略，保护下游资源不被耗尽，拖垮系统
设置适当的超时时间和重试策略，超时时间可以根据下游系统的tp99来设置，
超时时间的设置要遵循漏斗原则，上层系统的超时时间要大于底层系统的超时时间
2.4 接口重试
偶尔抖动的重试可以提高系统的可用性，而如果下游系统出现故障，重试可能会增加下游系统的负载，增加故障的严重程度
在一个rpc请求时，如果一个链路的rpc调用每个节点都设置重试，可能会引起重试风暴
同时如果是写接口，要保证接口的幂等性
2.5 接口的兼容性
老功能做功能迭代时，一定要做好兼容性
向前兼容性，旧版本的软件能够兼容新的数据和流量
向后兼容性：新版本的软件能够兼容旧的数据和流量
换而言之，就是要做好新数据&老系统 老数据&新系统这两种场景的兼容
由于要保证灰度发布和异常问题可回滚，所以必须要保证这两种场景都能正常处理
2.6 问题隔离
2.6.1划分系统
系统分为在线系统，离线系统 近实时系统（用户触发，异步执行）
2.6.2 线上环境，测试环境隔离（包括系统和中间件）
2.6.3 系统分级 按重要程度区分系统，系统异常时优先保证重要系统
读写隔离
线程池隔离
3 代码review
形成团队代码风格，风格统一，每次提交代码不要过多
4 上线 如何保证系统的稳定性
发生系统故障一般发生在上线阶段，包括代码部署,数据库变更，配置中心变更，一个系统不可能不出线上问题，我们所要追求的是
降低线上故障频率，缩短故障恢复时间：可监控，可灰度，可回滚
4.1 可监控
如果没有监控，上线过程中我们对系统的状态一无所知，监控指标分为业务指标和技术指标，技术指标又分为软件和硬件
业务指标：观测业务变化情况的度量，如订单量，支付量 
技术指标中的软件：可用率 tp99 调用量
技术指标中的硬件：cpu 内存，磁盘io 网络io
设置阈值，进行监控告警，监控可以先紧后松，避免遗漏告警，后续随着系统建设的迭代需要设置更合理的告警阈值，避免告警泛滥，
上线时做好指标监控和日志监控
4.2 可灰度
机器维度，机房维度，地域维度，业务维度
4.3 可回滚
出现问题，需要优先止损，其次才是分析原因
分为代码回滚和数据回滚，一般代码回滚即可，数据回滚比较困难
5 线上问题
5.1常见的线上问题
业务逻辑错误
    并发问题
    幂等问题
    状态机问题
    线程池问题
    full gc
存储
    数据库：
        主从延迟
        慢sql
        大事务
        死锁
        分页错误
    缓存
        缓存击穿
        缓存穿透
        缓存雪崩
        大key
        热key    
        数据一致性
    es 
        分片设置不合理
        分片数量超过集群上限
        慢查询
        写入后立即查
        一次写入数据量过多，过大
中间件
    升级
    消息堆压
    没做降级
安全
    越权访问
    sql注入
    DDos攻击
流程规范化

5.2 线上问题应对
问题发生->问题发现->问题处理—>问题恢复->问题复盘
问题预防：发现问题于未然
    明确需求，充分自测，code review,QA测试，线上回归
    研发规范：代码规范，日志规范，数据库规范，分支规范，依赖规范，
    变更流程：业务低峰变更，上线check list double-check，日志指标监控，适当增加降级开关，做好回滚预案
问题发现：
    自我意识：通过监控核心接口的不规律跳点，毛刺进行核心可用率，性能，调用量的review，同时也会中间件进行reveiw
    监控告警：必要的业务和技术指标
    业务反馈：等到业务反馈，说明已经影响到用户，这通常是因为监控的缺失
问题处理：
    保留现场，将现场的日志，数据等信息保存好，如内存dump，线程dump，防止机器重启后丢失信息
    提供信息：提供自己知道的信息，协助排查，不放大和缩小问题
    服务恢复：回滚，重启，扩容，禁用节点，功能降级
    双重确认：看监控各项指标是，数据，日志是否正常
    故障通告：通告业务人员，产品经历，系统的上下游，测试，运维等
问题定位（知识，工具，方法）：
    知识：不断学习full gc发生原因和解决方式
    工具：优先使用公司层面中间件
        日志：lombok grafana  
        链路追踪：skywalking
        监控工具：普罗米修斯
        jvm问题诊断：阿里的那个工具
        数据同步：canal
        配置中心：阿里的那个配置中心
        分布式id服务：xxx 
        任务调度：xxljob
        限流，降级：忘记了
    方法：
        日志监控法
        模拟法：本地复现
        检索法：异常日志网络检索
        求助法：短时间未解决则向上反馈
        排除法：根据其他性能正常的指标进行排除
        分治法：缩小问题边界
问题修复：最小改动->测试环境验证->周知qa和check list->灰度发布->线上回归->观察指标和告警->周知人员
问题复盘：
5why分析法剖析问题的根因
# 消息队列堆压问题 
https://juejin.cn/post/7499089664566657050
生产者生产消息速度大于消费者消费消息速度
1 发生原因：
 消费逻辑性能不足：消费者逻辑复杂或效率低下
 系统bug：消费者因bug导致消费能力下降或停止
 资源瓶颈：消费者资源不足，或topic队列数量限制了并行处理速度
 突发流量：生产者短时间产生大量消息，超出消费者能力
2 排查消息堆压问题
2.1 确认是否为bug导致
检查消费者日志：查看是否有异常或错误堆栈，确认是否存在代码逻辑问题
监控消费速度：通过控制台或监控工具，观察消费者消息速度是否异常
检查生产者速率：对比生产者发送速率和消费者处理速率，确认是否为流量突发
验证堆积问题：通过消费者的offset和消费进度，查看积压消息的时间跨度，判断是短期堆积还是长期堆积
如果发现bug（死循环，异常堵塞），需优先修复bug并验证其消费能力恢复
2.2 分析系统资源  
消费者机器资源：检查cpu，内存，磁盘IO，网络带宽等是否达到瓶颈
队列分配：确认topic队列是否足够，是否因队列不均导致的部分消费者空闲 
broker性能：检查broker的磁盘性能，网络吞吐量是否成为瓶颈
2.3 验证消费瓶颈
单条消费效率：分析消费者处理单条消息的耗时，是否存在复杂计算或外部依赖（数据库，网络请求）
并发能力：检查消费者是否利用了多线程或异步处理机制
3 常规优化方案
如果因为非bug引起，可以通过优化消费逻辑或水平扩容来解决
3.1 优化消费逻辑
批量消费：设置批量消费数量，减少网络io，需要注意不要拉取过多防止系统OOM
耗时操作（数据库操作，网络调用）异步处理
消息过滤：通过rocketmq的tag机制，过滤无效消息，减少消费者处理无关消息的开支
3.2 水平扩容
增加topic队列数
队列数决定了消息的并行度，可通过rocketmq或api动态增加队列数
命令示例：
mqadmin updateTopic -n <nameServer> -c <ClusterName> -t <topicName> -q <NewQueueNum>
增加队列数后，需确认broker磁盘和网络资源能支撑更高的吞吐量
多消费组
对于可以解耦的业务，设置多消费者，提升消息处理速度
3.3 生产者限流
生产者设置延时，或设置令牌桶，延迟处理消息
4 紧急方案处理（bug导致的严重积压） 
4.1: 暂停消费者处理（使用管理者工具停止消费，或结束消费者进程） ,修复bug（死锁，异常等）并验证
4.2: 创建临时topic，队列数为原来10倍：
mqadmin updateTopic -n <NameServer> -c <ClusterName> -t tempTopic -q 40, 并配置权限，确保生产者和消费者能够处理
4.3： 分发积压消息
部署一个临时的消费者程序，分发到临时的生产者中，消费这些堆积的消息
为临时topic部署10倍数量的消费者，每个消费者订阅一个队列，确保消费者资源充足
4.4 消息处理完成后，重新启动原消费者，继续处理消息，停止转发程序，临时消费者处理完消息后，删除消费者
5 预防措施
消费监控：完善消息报警机制，实时监测消息堆压量和消费延迟
自动化扩容：实现消费者和队列的动态扩容机制，应对突发流量
压测验证：定期对消费者进行压力测试，确保其能应对突发流量
降级方案：为关键业务设计降级策略，保证关键业务高可用

# 说说你的项目难点
https://www.nowcoder.com/discuss/648948173246844928
1技术难题
1.1 高并发请求
添加缓存来缓存热点数据
限流和降级：使用令牌桶和漏桶算法限制单位时间请求数量，监测到某个服务异常时，停止访问，仅返回关键信息
异步处理：rocketmq和kafka来处理异步任务，缓解即时处理的压力
数据库优化：读写分离，分库分表，分布式数据库来解决
1.2 数据和缓存一致性问题
使用延时双删解决
使用canal来监听binlog，更新到mq中，在通过mq消费来更新缓存
1.3 消息丢失、积压
消息丢失：使用消息确认机制，持久化，多机部署
消息积压：扩展消费者实例，优化消费者代码，限制生产者速率

2 线上调试难题
间歇性问题
间歇性图片覆盖问题
A用户生成图片时，发现是B用户的图片：使用的是时间戳，但是时间戳有可能重复，数据覆盖 图片命名添加上账号id
间歇性查询效率低问题
增加告警，监控，必要日志，看是数据库资源，内存资源，CPU资源线程资源引起的
间歇性OOM问题
配置参数，OOM时保存堆dump文件，下载后进行内存分析，看是哪些类内存占用过多，根据诊断的OOM问题，优化对应的代码
调整堆大小，新生代占比，垃圾回收器等措施

3 性能问题
程序运行时效率低下，相应缓慢，资源消耗过高或无法满足预期处理速度和吞吐量等情况
3.1程序性能问题：
3.1.1 高CPU：某些计算逻辑耗费机器性能
3.1.2 内存泄漏：未正确释放不使用的内存
3.1.3 频繁I/O操作：大量文件读写，网络请求
3.1.4 数据库查询性能差：不合理sql，索引
3.1.5 算法和数据结构使用不当
3.1.6 线程竞争和死锁：多线程资源竞争和死锁
解决方案：
性能分析：使用JProfier 监测程序性能指标，找出性能瓶颈
代码优化：优化代码和数据结构
内存管理：释放不再使用的内存资源
I/O优化: 使用异步IO，避免堵塞，对文件读写和网络请求批处理
数据库优化：优化sql查询
3.2 数据库性能问题
优化查询语句
建立优化索引
调整数据库配置：内存缓存区，连接数，线程池
分库分表：数据量过大时，垂直水平分表
事务：减少事务粒度，减少锁的持有时间
监控性能：定期监控数据库运行性能

# 应对高qps系统的可用性保障策略
1 体系介绍
1.1 流量入口层：
部署多机房多活架构
使用LVS/nginx实现四层、七层负载均衡
实施边缘计算（CDN）减轻资源压力
1.2 应用服务层
服务无状态设计，支持水平扩展
线程，连接池优化配置
实现请求级熔断（hystrix, sentinel）
1.3 数据访问层
多级缓存架构（本地缓存+分布式缓存）
数据库读写分离+分库分表
异步化设计（mq解耦）
2 关键技术
2.1 流量控制
分布式限流（redis， guava ratelimiter）
动态限流（基于系统负载均衡自动调整）
流量调度（DNS/GSLB实现流量分配）
2.2 缓存策略
热点数据预加载
缓存击穿，缓存雪崩防护
多级缓存过期策略
2.3 数据库优化
sql优化+慢查询监控
连接池优化（最大连接数+超时设置）
3 容灾与灾备
3.1 故障自愈：
自动扩缩容（k8s HPA）
服务降级预案
3.2 监控体系：
全链路监控
智能告警（动态阈值）
容量规划系统
3.3 预案演练
定期压测（影子流量，全链路压测）
故障演练（主动注入故障）
# 那如果现在你找运营去申请机器，他告诉你我没有这么多机器，嗯，就是 10 台。嗯，那你怎么去管这个主播呢
当前的核心矛盾是：
约束条件：机器无法扩容（10台固定）
目标要求：在现有资源下保持服务的稳定性：（1w+qps）
隐藏需求：证明资源投入的合理性（成本收益最大化）
问题类型：
技术问题：如何通过非硬件扩容手段维持服务？
管理问题：如何用数据说服团队支持资源决策？
1 资源利用优化
方向：榨干现有机器的性能
措施：
    服务画像：通过APM工具（如arthas）找出CPU/内存热点
    线程池调优：根据业务类型调整I/O密集型、CPU密集型的线程配置
    JVM优化：根据Java服务调整GC策略（如用G1替换CMS），堆内存分区
    内核参数：优化TCP backlog，文件描述符等系统参数
2 流量控制
让有限资源保障核心业务
措施：
    分级熔断：非核心功能高负载时自动降级
    智能限流：按用户等级差异化限流
    滑动窗口算法应对突发容量
    请求预处理：nginx层拦截恶意请求
3 架构改造
使用多级缓存：本地缓存（caffeine ）分布式缓存（redis），减少db压力
缓存预热：对热点数据主动预热
异步化：将同步调用改为队列，写操作批量提交
将CPU密集型任务转移到客户端
4 容灾兜底
超阈值时返回静态页面
流量调度：部分流量引流到备用集群

5 成本收益（说服决策层）
5.1 资源投入量化：单机QPS上限，tp95响应时间，cpu
5.2 业务价值证明
计算优化后的价值收益
机器成本：400w元/年
人力成本 80w 30人月
5.3 风险对冲
承诺优化后核心链路可用性99.95%
提供自动熔断后的补偿方案（发放优惠券）
执行建议：
技术侧：1周时间实施快速见效的优化（线程池调优+热点缓存）
数据侧：搭建监控看板，展示优化前优化后的对比数据
沟通测：向运维、决策层提供资源效益分析报告：各项措施优化的ROI对比，不同QPS下的资源预测

# 如何评估系统需要增加多少台机器
在我们的系统中，扩容决策是全面的监控数据和性能指标分析，当QPS达到1w时，我会首先检查CPU利用率，内存压力，I/O等待时间和
请求延迟等关键指标，特别是关注tp99延时是否仍在SLA范围内，以及错误率是否有异常上升 
扩容的触发不仅仅是单纯看QPS数字，而是当这些指标显示接近饱和状态，且通过垂直扩展（增加单机配置）无法有效解决问题时，
例如，如果发现CPU持续高于75%，请求延时TP99持续高于200ms，同时有请求被丢弃或超时，这是就需要考虑水平扩展
50台机器的评估基于我们的容量规划模型：通过压测我们连接到单机在合理的延迟下能处理250QPS，要稳定处理1w qps，理论需要40台，
我们通常增加25%的冗余应对突发容量和故障转移，所以扩展到50台，这个数字还会结合业务增长预测和成本考虑进行微调

# 如何评估系统能不能应对突发流量
1 当前系统基准评估
当前QPS和并发连接数
各个服务层的资源使用情况（CPU/内存/网络IO/磁盘IO）
数据库的负载情况
缓存命中率
2 压测
全链路压测，模拟千万用户场景
性能指标计算：计算单机承载能力（例如单机支持10w并发）
100台理论可支持1千万，但需考虑冗余70%
瓶颈分析：
识别系统瓶颈：可能是数据库，缓存，带款等
检查各项服务的水平扩展能力
3 扩容方案：
自动扩容（系统支持自动伸缩）
预热扩容（当前扩容到预估容量）
关键组件强化：
数据库：读写分离，分库分表
缓存：使用redis集群
CDN：确保资源静态分发能力
4 降级方案
非核心功能暂时关闭
限流策略设置（API网关层，服务层）
排队应对瞬时高峰
5 监控与应急
加强实时监控（各层指标，业务指标）
准备回滚方案
# 如何做线上压测
1 压测前准备
1.1 选择合适的时间窗口
压测在业务低峰期进行
提前分析与生产环境的错峰时间段，确保压测流量与真实用户错峰
1.2 压测数据隔离
数据染色：通过标记压测流量（在http请求加tag，在rpc请求加压测标识）区分压测与真实数据
影子库/表：将压测数据写入独立的影子库或表，避免污染生产数据
专用测试账号：特殊格式的用户id或账号，模拟用户行为
1.3 系统改造和验证
压测开关：服务段支持动态开关压测流量，未开启时拒绝压测请求
第三方服务mock：对不支持外部服务进行mock，避免触发真实业务逻辑
链路验证：确保压测流量能透传到下游服务

2 压测执行策略
2.1 流量生成方式
流量回放：录制线上真实请求（如nginx日志），按比例回放至压测环境，适用于读操作
写请求模拟：构造写请求需确保数据隔离
实时引流：从线上分流部分流量至压测集群（控制比例，避免过载）
2.2 渐进式加压
从低并发开始（如10%），逐步增加压力，观察系统指标（CPU 响应时间 错误率）
2.3 全链路压测与熔断
监控指标：实时跟踪QPS，响应时间，资源利用率，错误率
熔断机制：预设阈值（如响应时间大于5s，错误率大于1%），触发后停止压测

3 压测后处理
3.1 数据清理
根据压测标记清理影子数据和测试数据
3.2 复盘与优化
分析压测报告，识别瓶颈（数据库慢查询，服务线程不足），并优化
总结：
线上压测的核心是通过流量隔离，数据隔离和渐进式加压实现安全评估，需结合业务特点选择工具（流量回放或实时引流），
并依赖完善的监控与熔断机制保障稳定性

# 机器资源有限时的替代方案
1 消费者服务优化：优化消费者逻辑，提高单机处理能力
2 实施精细化的线程池和连接池管理
3 增加kafka分区数量，提高并行消费能力
4 调整消费者组配置，优化负载均衡
5 优化网关层，应用层，多级限流
6 准备降级方案，极端情况考虑核心功能
7 非核心功能进一步异步化
8 增加监控，实现自动扩缩容 
9 增加熔断策略，保证核心功能

# 用户和内存增长是纯线性关系吗
在用户行为均匀且无共享资源的场景下，初期可近似为线性关系，便于快速估算，实际生产环境中，因缓存，连接复用，锁资源征用
等，内存增长呈线性或阶段性跃升，需要结合历史数据和压力测试，建立更准确的非线性模型

# 然后前台提到就是主播这边业务比较复杂，对吧？是，嗯，这个业务复杂度体现在哪呢
1 实时性要求高
设计高效的心跳检测机制
使用rocketmq的延时队列，接收到心跳后放入到rocketmq中
2 多系统的协调
用户行为埋点（记录停留时长）
实时推荐系统（生成推荐结果）
直播状态服务
A/B测试平台（进行策略验证）
3 策略复杂度
频控策略：同一用户短时间不会重复触发
场景策略：用户送礼，发言互动时不打扰
分级策略：不同用户看到的推荐权重不同
4 性能与体验的平衡
推荐结果预加载，保证点击后流畅跳转
降级方案：超时时直接隐藏
5 数据闭环
曝光埋点：记录弹框展示
点击埋点：记录用户选择
转化埋点：跟踪跳转后的观看时长
用这些数据持续优化推荐系统

# 高可用，高性能，高并发




